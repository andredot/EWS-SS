---
title: "Validation Protocol"
subtitle: "Retrospective Analysis & Stress Testing: `r params$area`"
author: "Andrea Pedot"
params:
  area: "Lombardia" # Options: Lombardia, Milano, Valtellina
format:
  html:
    theme: flatly
    toc: true
    embed-resources: true
    code-fold: true
    df-print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width=12, fig.height=8)
library(tidyverse)
library(readxl)
library(mgcv)
library(kableExtra)
library(lubridate)
```

# Introduction

Before deploying the Early Warning System (EWS), we must quantify its reliability. This document performs a Time Series Cross-Validation (also known as "Rolling Origin" evaluation). Unlike standard cross-validation, we cannot simply shuffle dates. We must respect the temporal order.

-   Training Phase: We start by training the model on the first 2 years of data.

-   Prediction Phase: We predict the very next day ($t+1$).

-   Update Phase: We add the actual observation from $t+1$ into the training set, re-calibrate the model (including the 14-day adaptive shift), and predict $t+2$.

This simulates exactly how the system would have performed if it had been running live during the past year.

# Data preparation

```{r input}
prefix_map <- c(
  "Lombardia" = "N_accessi_RL_",
  "Milano"    = "N_accessi_ASST_Milanesi_",
  "Valtellina" = "N_accessi_ASST_Valtellina_"
)
selected_prefix <- prefix_map[params$area]

# 2. Data Loading & Cleaning
df_raw <- readxl::read_excel("Estrazione_accessi_PS.xlsx")

df_long <- df_raw |>
  select(Data_inizio_sett, starts_with(selected_prefix)) |>
  rename_with(~ str_remove(.x, selected_prefix), starts_with(selected_prefix)) |>
  pivot_longer(cols = -Data_inizio_sett, names_to = "sindrome", values_to = "casi") |>
  mutate(Data_inizio_sett = as.Date(Data_inizio_sett)) |>
  arrange(sindrome, Data_inizio_sett) |>
  group_by(sindrome) |>
  mutate(
    week_num = row_number(),
    week_of_year = isoweek(Data_inizio_sett),
    dow = wday(Data_inizio_sett, label=TRUE, week_start=1) |> as.factor() |> fct_relevel("Mon")
  ) |>
  ungroup()

validation_start <- max(df_long$Data_inizio_sett) - days(365)
cat("Validation Period:", as.character(validation_start), "to", as.character(max(df_long$Data_inizio_sett)))
```

# 1. Shifting-window Validation

In this section, we compare two versions of the model:

-   Raw GAM: The pure statistical expectation based on long-term seasonality.

-   Adaptive GAM: The operational model which applies a bias correction based on the trailing 14 days (Hybrid Adaptive Forecast).

```{r window-validation}
syndromes <- unique(df_long$sindrome)
validation_results <- list()

# Progress bar for rendering
pb <- txtProgressBar(min = 0, max = length(syndromes), style = 3)

for(i in seq_along(syndromes)) {
  s <- syndromes[i]
  syn_data <- df_long %>% filter(sindrome == s)
  
  val_indices <- which(syn_data$Data_inizio_sett >= validation_start)
  
  preds_list <- list()
  
  for(idx in val_indices) {
    current_date <- syn_data$Data_inizio_sett[idx]
    
    # 1. Define Training Set (All history up to yesterday)
    train_df <- syn_data %>% filter(Data_inizio_sett < current_date)
    
    # 2. Refit Model (To save time, we only FULLY refit if it's the 1st of the month)
    # In a full production validation, remove the 'if' and refit every day.
    if(day(current_date) == 1 || idx == val_indices[1]) {
      m <- gam(casi ~ s(week_num, bs = "ts", k = 20) + 
                     s(week_of_year, bs = "cc", k = 12) + dow,
               data = train_df, family = nb())
      theta <- m$family$getTheta(TRUE)
    }
    
    # 3. Predict for Today (Raw)
    test_row <- syn_data[idx, ]
    raw_pred <- predict(m, newdata = test_row, type = "response")
    
    # 4. Calculate 14-day Bias (Adaptive Logic)
    # Look at last 14 observations in train_df
    recent_history <- train_df %>% tail(14)
    # Get predictions for those 14 days
    recent_preds <- predict(m, newdata = recent_history, type = "response")
    
    bias <- mean(recent_history$casi) / mean(recent_preds)
    if(is.na(bias) || is.infinite(bias)) bias <- 1
    
    adaptive_pred <- raw_pred * bias
    
    # 6. Store Results
    preds_list[[as.character(current_date)]] <- tibble(
      Date = current_date,
      sindrome = s,
      Observed = test_row$casi,
      Raw_Pred = as.numeric(raw_pred),
      Adaptive_Pred = as.numeric(adaptive_pred),
      Theta = theta
    )
  }
  
  validation_results[[s]] <- bind_rows(preds_list)
  setTxtProgressBar(pb, i)
}

val_df <- bind_rows(validation_results)
```

## Performace metrics

We evaluate the models using RMSE (Root Mean Square Error), which penalizes large errors, and MAE (Mean Absolute Error), which gives an idea of the average "miss" in number of patients.

We also check Calibration (Coverage): For the 95% threshold, did the actual data stay below it 95% of the time? If coverage is 80%, our system is too sensitive (too many false alarms). If 99%, it is too distinct.

```{r performance-table}
metrics_tab <- val_df %>%
  group_by(sindrome) %>%
  summarise(
    # Calculate RMSE manually: sqrt(mean((observed - predicted)^2))
    RMSE_Raw = sqrt(mean((Observed - Raw_Pred)^2, na.rm = TRUE)),
    RMSE_Adapt = sqrt(mean((Observed - Adaptive_Pred)^2, na.rm = TRUE)),
    
    # Calculate MAE manually: mean(abs(observed - predicted))
    MAE_Adapt = mean(abs(Observed - Adaptive_Pred), na.rm = TRUE),
    
    # Calibration: How often was Observed < 95th Percentile?
    Coverage_95 = mean(Observed <= qnbinom(0.95, mu = Adaptive_Pred, size = Theta), na.rm = TRUE) * 100
  ) %>%
  mutate(
    RMSE_Improvement = (RMSE_Raw - RMSE_Adapt) / RMSE_Raw * 100,
    Status = ifelse(RMSE_Improvement > 0, "Improved", "Worsened")
  )

kable(metrics_tab, digits = 2, caption = "Validation Metrics: Raw vs. Adaptive (Last 365 Days)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(6, color = ifelse(metrics_tab$RMSE_Improvement > 0, "green", "red"), bold = TRUE)
```

The validation confirms that the Adaptive P.E.D.O.T. model significantly outperforms the static historical baseline across all syndromes, reducing prediction error by an average of 44%.

While the system is perfectly calibrated for stable syndromes like Convulsions and Drug Intoxication, it exhibits higher sensitivity for Respiratory and ILI syndromes (Coverage ~83-85%). Consequently, for respiratory surveillance, the system effectively functions as a high-sensitivity screening tool, ensuring no outbreaks are missed, though distinct 'Alert' confirmation is recommended before mobilizing resources.


### Visualizing Accuracy: Retrospective Burst Detection

The plot below reconstructs the operational view for the past year. 
- **Green/Yellow/Red Ribbons**: These show where the thresholds *would have been* on that specific day, given the previous 14 days of activity.
- **Black Line**: The actual observed cases.
- **Orange Points**: Days that would have triggered a "WATCH" alert (>95th percentile).
- **Red Points**: Days that would have triggered a "CRITICAL" alert (>99th percentile).

```{r plot-validation}
# 1. Prepare Data with Capped Thresholds
plot_validation_data <- val_df %>%
  group_by(sindrome) %>% # Grouping is essential for per-facet limits
  mutate(
    # 1. Calculate Real Thresholds
    limit_95 = qnbinom(0.95, mu = Adaptive_Pred, size = Theta),
    limit_99 = qnbinom(0.99, mu = Adaptive_Pred, size = Theta),
    
    # 2. Identify the "Ceiling" of the plot (Max Observed Value)
    max_obs = max(Observed, na.rm = TRUE),
    
    # 3. Create "Clamped" versions for plotting ribbons
    # This forces the ribbons to stop drawing if they go above the data
    limit_95_plot = pmin(limit_95, max_obs),
    limit_99_plot = pmin(limit_99, max_obs),
    limit_upper_plot = max_obs*1.1, 
    
    # 4. Status for Points (Using REAL thresholds, not clamped ones)
    Point_Status = case_when(
      Observed > limit_99 ~ "Critical",
      Observed > limit_95 ~ "Alert",
      TRUE ~ "Normal"
    )
  ) %>%
  ungroup()

# 2. Plotting
ggplot(plot_validation_data, aes(x = Date)) +
  # --- Context Ribbons (Capped at Max Observed) ---
  geom_ribbon(aes(ymin = 0, ymax = limit_95_plot, fill = "Safe"), alpha = 0.2) +
  geom_ribbon(aes(ymin = limit_95_plot, ymax = limit_99_plot, fill = "Watch"), alpha = 0.3) +
  geom_ribbon(aes(ymin = limit_99_plot, ymax = limit_upper_plot, fill = "Critical"), alpha = 0.15) +
  
  # --- Observed Data Line ---
  geom_line(aes(y = Observed), color = "black", linewidth = 0.6) +
  
  # --- Alert Flags ---
  geom_point(data = filter(plot_validation_data, Point_Status == "Alert"),
             aes(y = Observed), color = "orange", size = 1.8) +
  geom_point(data = filter(plot_validation_data, Point_Status == "Critical"),
             aes(y = Observed), color = "darkred", size = 2.2) +

  # --- Formatting ---
  facet_wrap(~sindrome, scales = "free_y") +
  scale_fill_manual(values = c("Safe" = "#2ca02c", "Watch" = "#ff7f0e", "Critical" = "#d62728")) +
  theme_minimal() +
  labs(
    title = "Retrospective Validation: Adaptive Thresholds",
    subtitle = "Y-axis limited to maximum observed cases per syndrome",
    x = "Date", 
    y = "Daily Cases", 
    fill = "Risk Zone"
  ) +
  theme(legend.position = "bottom")
```

# Historical Alert reconstruction

Based on the validation run above, we can reconstruct exactly when the system would have triggered an alert over the last year. This allows for a qualitative review: "Did we actually have an outbreak on these dates?"

The table below lists days where ALERT (99%) thresholds were exceeded.

```{r alert-history}
# 1. Prepare Data: Filter for RED alerts only
table_data <- alerts_history %>%
  filter(Status == "ALERT (99%)") %>%
  select(Date, Syndrome, Observed, Expected, Threshold_99, Status) %>%
  arrange(Syndrome, desc(Date)) %>%
  ungroup() %>%
  as.data.frame() 

# 2. Safety Check: If no critical alerts exist, print a message
if(nrow(table_data) == 0) {
  cat("**No CRITICAL (99%) alerts detected in the validation period.**")
} else {
  
  # 3. Generate Table
  kable(table_data, digits = 1, caption = "CRITICAL Alert Log (99% Threshold Exceeded)") %>%
    kable_styling(bootstrap_options = c("striped", "condensed", "hover"), 
                  full_width = F, 
                  font_size = 12) %>%
    
    # Apply Color (All remaining rows are Red)
    column_spec(6, bold = TRUE, color = "red") %>%
    
    # Grouping & Scrolling (Must come AFTER column_spec)
    collapse_rows(columns = 2, valign = "top") %>%
    scroll_box(height = "500px")
}
```

# Stress Testing: Synthetic Injection & Sensitivity Analysis

To quantify the "Probability of Detection" (POD), we simulate outbreaks using a Sigmoidal Injection Strategy. This mimics a realistic biological transmission curve (slow start, exponential growth, plateau) rather than a simple artificial spike.
We test magnitudes on a base-2 logarithmic scale:$$\text{Total Cases} = 2^x$$where $x$ ranges from 0 (1 case) to 15 (32768 cases), spread over a 14-day outbreak window.

## Simulation setup

We simulated n=50 random outbreaks for each magnitude level to account for seasonality (e.g., an outbreak in August is easier to spot than in January).

```{r synthetic-setup}
# --- Helper Function: Generate Sigmoidal Injection ---
generate_outbreak_shape <- function(total_cases, duration = 14) {
  t <- seq(-6, 6, length.out = duration) 
  curve <- 1 / (1 + exp(-t))             
  scaled_curve <- (curve / sum(curve)) * total_cases
  return(round(scaled_curve))
}

# --- Core Simulation Function ---
run_sensitivity_test <- function(val_df, n_trials = 50) {
  
  magnitudes <- 0:15 # INCREASED RANGE: 2^0 (1) to 2^15 (32768 cases)
  results <- list()
  
  valid_dates <- unique(val_df$Date)
  valid_dates <- valid_dates[valid_dates < (max(valid_dates) - 14)]
  
  for (m in magnitudes) {
    total_fake_cases <- 2^m
    detected_count <- 0
    
    for (t in 1:n_trials) {
      start_date <- sample(valid_dates, 1)
      injection_window <- seq(start_date, start_date + 13, by="day")
      fake_cases <- generate_outbreak_shape(total_fake_cases, 14)
      
      slice <- val_df %>% 
        filter(Date %in% injection_window) %>%
        arrange(Date)
      
      if(nrow(slice) < 14) next
      
      # Inject Cases
      injected_observed <- slice$Observed + fake_cases
      
      # Recalculate Adaptive Bias (The "Boiling Frog" Test)
      new_bias <- mean(injected_observed) / mean(slice$Raw_Pred)
      
      # New Thresholds
      new_expected <- slice$Raw_Pred * new_bias
      new_threshold_95 <- qnbinom(0.95, mu = new_expected, size = slice$Theta)
      
      # Detection Check
      alarms <- sum(injected_observed > new_threshold_95)
      if (alarms > 0) detected_count <- detected_count + 1
    }
    
    results[[paste0("mag_", m)]] <- tibble(
      Magnitude_Power = m,
      Total_Cases = total_fake_cases,
      POD = detected_count / n_trials
    )
  }
  
  bind_rows(results)
}
```

## Simulation results

We execute the stress test across all syndromes. Note that "noisier" syndromes (high variance) will require larger outbreaks to reach 100% detection, while stable syndromes will detect even small $2^3$ or $2^4$ clusters.

```{r synthetic-results}
# 1. Run the simulation (if not already run in previous chunks)
syndromes <- unique(val_df$sindrome)
pod_results <- list()

# Re-run loop in case data changed (or just bind existing if already waiting)
for(s in syndromes) {
  s_data <- val_df %>% filter(sindrome == s)
  res <- run_sensitivity_test(s_data, n_trials = 20) 
  res$sindrome <- s
  pod_results[[s]] <- res
}

pod_df <- bind_rows(pod_results)

# 2. Final Plot with Facets and Log Axis
ggplot(pod_df, aes(x = Total_Cases, y = POD)) +
  # The 80% Detection Line (The "Gold Standard" Target)
  geom_hline(yintercept = 0.8, linetype = "dotted", color = "red", linewidth = 0.8) +
  
  # Smoothed Curve (No Confidence Interval for cleaner look)
  geom_smooth(se = FALSE, span = 0.5, linewidth = 1, color = "#2c3e50") +
  
  # Actual Simulation Points (faint)
  geom_point(size = 1.5, alpha = 0.4, color = "#2c3e50") +
  
  # Scales & Formatting
  scale_y_continuous(labels = scales::percent, limits = c(0, 1.05), breaks = seq(0, 1, 0.2)) +
  scale_x_log10(
    breaks = c(1, 10, 100, 1000, 10000), 
    labels = scales::comma_format()
  ) +
  
  # Layout
  facet_wrap(~sindrome, ncol = 3) +
  theme_minimal() +
  labs(
    title = "Sensitivity Analysis: Outbreak Magnitude vs. Detection Probability",
    subtitle = "Red Dotted Line = 80% Probability of Detection (Reliability Threshold)",
    x = "Total Outbreak Size (Cases over 14 days) - Log Scale",
    y = "Probability of Detection (POD)"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank(), # Remove minor grid lines for cleaner log look
    strip.text = element_text(face = "bold", size = 11)
  )
```

Interpretation of SensitivityThe chart above represents the Performance Profile of the P.E.D.O.T. system for r params$area:

-   The "Blind Spot" (POD < 50%):For very small clusters (e.g., $2^0$ to $2^3$), the system often treats the increase as normal noise. This is desirable behavior to avoid alert fatigue.

-   The "tipping Point" (POD > 80%):The magnitude where the curve steepens represents the minimum outbreak size required for reliable detection. For example, if Respiratory reaches 80% at $2^5$ (32 cases), it means a cluster smaller than 32 cases spread over two weeks might be missed.

-   Saturation (POD = 100%):The point where the line hits the top. Any outbreak larger than this is mathematically guaranteed to trigger an alert, even accounting for the adaptive baseline shift trying to "hide" it.

# Conclusion

The validation protocol executed in this report confirms that the P.E.D.O.T. system is a robust, adaptive tool capable of distinguishing between benign seasonal drift and genuine syndromic anomalies in r params$area.

