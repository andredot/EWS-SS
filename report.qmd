---
title: "Early Warning System for Syndromic Surveillance"
subtitle: "Predictive Epidemiological Detection & Outbreak Tracking"
author: "Andrea Pedot"
params:
  area: "Lombardia" # Options: Lombardia, Milano, Valtellina
format:
  html:
    embed-resources: true
    theme: flatly
    toc: true
    toc-depth: 2
    code-fold: true
    df-print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width=12, fig.height=10)
```

## Introduction

The primary objective of this report is to establish a robust Early Warning System (EWS) for syndromic surveillance across Lombardy region. By monitoring weekly Emergency Department (ED) visits for nine key syndromic diseases over a three-year period, we aim to distinguish between expected seasonal fluctuations and true anomalous spikes that may indicate a public health threat.

To achieve this, the report evaluates and compares two statistical methodologies designed to handle the complexity of public health data:

-   Approach A: Negative Binomial Quantiles – This method utilizes the intrinsic probability distribution of the data to define dynamic thresholds. It identifies anomalies by determining where the current week's count falls relative to the expected 95th and 99th percentiles.
-   Approach B: Standardized Residuals (Sigma Scores) – This approach measures the statistical "distance" between observed and expected counts. By calculating a standardized score, it quantifies how many standard deviations the current volume deviates from the model’s prediction, providing a clear measure of conviction for each alert.

By integrating these models, the system filters out recurring seasonal "noise" allowing public health experts to focus on significant, unexplained deviations in real-time.

```{r libraries}
library(tidyverse)
library(mgcv)       
library(kableExtra)
```

## Data import e preprocessing

The raw dataset is transformed into a "Tidy" long-format structure to facilitate syndrome-specific modeling. During this phase, we standardize column names and compute the weekly delta ($\Delta$), representing the change in case volume from the previous seven days. To support the non-linear modeling in the GAM phase, we derive several temporal features: `week_num` serves as a continuous time index to capture long-term trends, while `week_of_year` and `dow` (Day of the Week) are extracted to account for annual seasonality and potential reporting patterns.

```{r import}
all_data <- tar_read(all_data_list)
df_long <- all_data[[params$area]]

cat("### Analysis for Area: ", params$area)
```

## Exploratory Analysis

The exploratory phase highlights distinct, syndrome-specific dynamics. High-volume syndromes, such as respiratory and flu-like illnesses, exhibit significant seasonal variance, while rare syndromes like intoxications cluster near zero but are prone to sudden, overdispersed spikes.

### Long-term and Seasonal Trends

By visualizing the weekly changes ($\Delta$), we can observe how the volatility of each syndrome evolves over time. These plots help identify multi-year trends and recurring seasonal peaks, which the GAM will eventually use to build the "expected" baseline for any given week.

```{r seasonal-patterns}
df_long |>
  ggplot(aes(week_num, delta_casi, 
             color = sindrome)) +
  geom_point(alpha=0.05) +
  geom_smooth()+
  facet_wrap(~sindrome, scales="free_y") +
  labs(title="Long-term Trends in Weekly Δ", x="Week Number (Chronological)")
```

### Weekly patterns

Day-of-week effects can reveal reporting biases or behavioral patterns (e.g., increased ED visits on Mondays following the weekend). Understanding these fluctuations ensures the model does not mistake a standard "Monday surge" for a disease outbreak.

```{r weekly-patterns}
df_long |>
  ggplot(aes(dow, delta_casi,
             color = sindrome)) +
  geom_boxplot() +
  facet_wrap(~sindrome, scales="free") +
  labs(title="Day-of-Week Effects on Δ")
```

### Overdispersion

Overdispersion occurs when the variance of the data significantly exceeds the mean. This is particularly prevalent in rare syndromes where long periods of zero cases are interrupted by occasional spikes. In such cases, standard Poisson models—which assume the mean and variance are equal—will underestimate the probability of extreme events, leading to excessive false alarms. An overdispersion index significantly greater than 1 indicates that Negative Binomial or quasi-Poisson approaches are statistically required.

```{r overdispersion}
all_stats <- tar_read(all_stats_list)
stats_current <- all_stats[[params$area]]

overdisp_stats <- stats_current$overdispersion

kable(overdisp_stats, digits=3, caption="Overdispersion by Syndrome (var/|mean| >1 = overdispersed)")
```

### Auto-correlation

Autocorrelation measures the "memory" of the data—how much last week's volume influences the current week. Significant autocorrelation (specifically at Lag-1) suggests epidemiological carryover, such as a localized outbreak that persists for several weeks. Identifying these dependencies allows us to distinguish between a series of expected high-volume weeks and a truly anomalous new spike.

```{r auto-correlation}
# Extract pre-calculated ACF data
acf_data <- stats_current$acf

ggplot(acf_data, aes(x = lags, y = acf_vals, color = sindrome)) +
  geom_hline(yintercept = 0, color = "gray50") +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  facet_wrap(~sindrome, ncol = 3, scales = "free_y") +
  labs(title = "ACF of Weekly Deltas - All Syndromes",
       x = "Lag (weeks)", y = "ACF") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
```

## Model Selection and Training

Generalized Additive Models (GAMs) serve as the core of this surveillance system because they excel at decomposing complex Emergency Department signals into interpretable components. By using thin-plate regression splines, the model identifies non-linear long-term trends in healthcare utilization, while cyclic cubic splines capture recurring annual seasonality—such as winter flu peaks—ensuring a smooth transition from the end of one year to the start of the next. Crucially, the system employs the Negative Binomial family to account for the "bursty" nature of syndromic data, where the variance often exceeds the mean.

In public health, "Normal" (Gaussian) distributions are rarely used for raw counts because case data is usually asymmetric (you can't have fewer than zero cases, but you can have a massive spike) and overdispersed.

### Baseline Models

To evaluate the added value of the GAM, we compare its performance against three simple benchmarks that establish a "performance floor." These baselines represent common, less complex methods for anomaly detection:

-   Gaussian Benchmark: Assumes a normal distribution of the weekly changes (Deltas) based on the previous 52 weeks. While it accounts for recent scale, it ignores non-linear seasonality and assumes variance is constant.
-   Historical Range (Rolling Min/Max): Provides temporal context by looking at the extreme values recorded over the previous 2 years. While intuitive, it is highly sensitive to past outliers.
-   Naive Poisson: A standard count model that frequently fails in syndromic surveillance because it cannot account for negative differences between weeks or the significant overdispersion found in real-world health data.

```{r baseline-models}
# Load pre-calculated baselines
all_baselines <- tar_read(all_baselines_list)
baseline_summary <- all_baselines[[params$area]]

kable(baseline_summary, digits = 1, caption = "Historical Benchmarks (Last 52 Weeks)") |>
  kable_styling(bootstrap_options = "striped", font_size = 11)
```

### Approach A: Negative Binomial Probabilities

This strategy models the absolute case volume using the Negative Binomial distribution to establish dynamic thresholds. Rather than looking at the difference ($Δ$) in isolation, it calculates the probability of the observed count occurring given the historical trend and seasonality. An anomaly is flagged when the observed count falls into the extreme 95th (Yellow) or 99th (Red) percentile of the predicted distribution. This method is particularly robust for rare syndromes where case counts are frequently near zero.

It answers to the question: "Statistically, how rare is this specific number?". - It looks at the entire shape of the Negative Binomial "bell" for this specific week. If the observed count falls into the final 1% of the tail, it triggers a Red Alert. - It is extremely accurate for low-volume syndromes (like Intoxications). It understands that moving from 0 cases to 2 cases might be a 200% increase, but it's still statistically "probable" noise.

### Approach B: Ratio to Expected counts

Approach B evaluates the "distance" between the observed counts and the model's expectation. By calculating the Standardized Pearson Residual, we derive a "Sigma Score" that accounts for overdispersion. This score quantifies how many standard deviations the current volume deviates from the mean. Thresholds are set at 1.96σ (95% confidence) and 2.57σ (99% confidence). This dual-model setup effectively filters out seasonal noise, highlighting true public health threats across all nine syndromes.

It answers to the question: "How many steps away from the average is this?".

```{r model-fit}
fit_surveillance_model <- function(df_subset) {
  # Reverting to the stable Option 1 configuration
  m <- mgcv::gam(casi ~ s(week_num, k = 50) + 
                       s(week_of_year, bs = "cc", k = 12) + 
                       dow, 
                 family = nb(), 
                 data = df_subset, 
                 method = "REML")
  
  theta <- m$family$getTheta(TRUE)
  preds <- predict(m, type = "link", se.fit = TRUE)
  
  results <- df_subset |>
    mutate(
      expected = exp(preds$fit),
      # Approach A: Negative Binomial Quantiles
      p_val = pnbinom(casi - 1, mu = expected, size = theta, lower.tail = FALSE),
      limit_95_A = qnbinom(0.95, mu = expected, size = theta),
      limit_99_A = qnbinom(0.99, mu = expected, size = theta),
      status_A = case_when(
        casi > limit_99_A ~ "RED",
        casi > limit_95_A ~ "YELLOW",
        TRUE ~ "GREEN"
      ),
      # Approach B: Sigma Scores
      std_residual = (casi - expected) / sqrt(expected + (expected^2 / theta)),
      status_B = case_when(
        std_residual > 2.57 ~ "RED",    
        std_residual > 1.96 ~ "YELLOW", 
        TRUE ~ "GREEN"
      )
    )
  
  return(list(model = m, data = results))
}

surveillance_output <- df_long |> 
  split(df_long$sindrome) |> 
  map(fit_surveillance_model)

results_list <- map(surveillance_output, ~ .x$data)
models_list <- map(surveillance_output, ~ .x$model)
results_df <- bind_rows(results_list)

# saveRDS(models_list, "lombardia_gam_models.rds")
```

### Note on interpretation

Employing both methodologies ensures the detection of various error types and provides a multi-layered validation of alerts: -*Consistency Validation* - When both Approach A and Approach B simultaneously trigger a "RED" status, the statistical confidence that a true outbreak is occurring is significantly strengthened through independent verification. - *Management of Low-Volume Data* - In syndromes characterized by rare events, Approach B (Residuals) may produce disproportionately high scores in response to minor absolute changes. Approach A (Quantiles) functions as a necessary "sanity check," ensuring that any alarm is justified by the actual probability distribution of the data. - *Strategic Prioritization* - In scenarios where multiple syndromes trigger a "YELLOW" status, the Sigma Scores from Approach B allow for objective prioritization. The syndrome with the highest Sigma Score represents the most significant deviation relative to its specific historical context, indicating where investigation should begin.

The combination of a Probability-based method with a Magnitude-based method results in a balanced early warning system. This integration maintains high sensitivity for early outbreak detection while remaining sufficiently stable to filter out routine seasonal noise.

When reviewing weekly alerts, it is essential to distinguish between random statistical fluctuations and emerging public health threats. Alerts with a Sigma Score \> 3.0 represent extreme deviations that are virtually impossible by chance alone, but monitoring nine syndromes simultaneously increases the probability of "False Red" alerts to approximately 8.6% per week due to the laws of probability, and a 37% Yellow alert. This means that roughly every three days, the system will flash "Yellow" somewhere, even if absolutely nothing is happening.

### Diagnostic plots

This section evaluates the structural integrity of the GAMs using a basis dimension ($k$) check. We compare the Effective Degrees of Freedom (EDF) against the maximum available knots ($k'$); an EDF significantly lower than the limit suggests the model has successfully identified an optimal smoothing level, while an EDF nearing the limit indicates a "ceiling effect" where the model is attempting to capture high-frequency noise or jagged spikes inherent in daily ED data. While low p-values often persist due to non-smooth human behaviors and reporting artifacts, they do not necessarily invalidate the model’s utility as a robust surveillance baseline, provided the overall trend captures the seasonal and weekly cycles effectively.

```{r diagnostic-loop, results='hold'}
for (s in names(models_list)) {
  cat("\n========================================================\n")
  cat("DIAGNOSTICS FOR SYNDROME:", toupper(s), "\n")
  cat("========================================================\n")
  
  mod <- models_list[[s]]
  if (inherits(mod, "gam")) {
    check_table <- capture.output(mgcv::k.check(mod))
    cat(paste(check_table, collapse = "\n"))
  } else {
    cat("Warning: Object for", s, "is not a valid GAM object.\n")
  }
  cat("\n")
}
```

## Model Validation

Before deploying these thresholds for health protection, the models undergo a rigorous validation process to ensure they outperform traditional historical baselines.

### Explanatory power and Empirical calibration

The GAMs are evaluated on their ability to explain the variance in syndrome counts. While a historical mean (baseline) often has low predictive power, these models typically achieve an Adjusted $R^2 > 0.70$. This indicates that over 70% of the fluctuations in case volume are successfully explained by known temporal and seasonal patterns, leaving only 30% as potential "noise" or "outbreaks."

A well-calibrated surveillance system should be "right about its own uncertainty." We validate this by checking the Empirical Coverage:

-   Test: Over a stable historical period, we calculate how many observed data points fall within the model's 95% prediction interval.
-   Goal: If the model is accurate, \~95% of observations should be classified as "Green".
-   Result: If significantly fewer than 95% are Green, the model is too sensitive (high false-alarm rate). If more than 95% are Green, the model is too "stiff" and may miss real outbreaks.

```{r calibration-stats}
# 1. Calculate Empirical Coverage
calibration_stats <- results_df |>
  group_by(sindrome) |>
  summarise(
    Actual_95_Coverage = mean(casi <= limit_95_A) * 100,
    Actual_99_Coverage = mean(casi <= limit_99_A) * 100,
    R_Squared = 1 - (sum((casi - expected)^2) / sum((casi - mean(casi))^2)),
    Green_Rate = mean(status_A == "GREEN"),
    Avg_Sigma = mean(std_residual)
  )

calibration_stats
```

```{r calibration-plot}
# 2. Create the Calibration Plot
# This plot compares Observed vs Expected counts with the 95% Prediction Interval
ggplot(results_df, aes(x = expected, y = casi)) +
  geom_point(alpha = 0.3, color = "midnightblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey50") +
  geom_smooth(method = "gam", color = "firebrick", se = FALSE) + # Trend of actuals
  facet_wrap(~sindrome, scales = "free") +
  theme_minimal() +
  labs(
    title = "Model Calibration: Observed vs. Expected Counts",
    subtitle = "The dashed line represents a perfect 1:1 fit. Red line shows the actual model trend.",
    x = "Model Expected Cases",
    y = "Observed Cases (Reality)"
  )
```

```{r qq-plot}
# 1. Calculate residuals from the existing results_df
qq_data <- results_df |>
  group_by(sindrome) |>
  mutate(
    # We use the standardized residual (Sigma Score) we already calculated
    # In a well-fit model, these should follow a Normal distribution
    observed = sort(std_residual),
    theoretical = qnorm(ppoints(n()))
  ) |>
  ungroup()

# 2. Generate the faceted plot
ggplot(qq_data, aes(x = theoretical, y = observed)) +
  geom_point(alpha = 0.4, color = "midnightblue", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "firebrick") +
  facet_wrap(~sindrome, ncol = 3, scales = "free") +
  theme_minimal(base_size = 11) +
  labs(
    title = "System-Wide Distributional Validation (Q-Q Plots)",
    subtitle = "Standardized residuals vs. Theoretical Normal quantiles.",
    x = "Theoretical Quantiles",
    y = "Observed Sigma Scores"
  )
```

### Benchmarking

We compare the GAM's performance against the Rolling two-year Baseline using three key metrics:

-   False Alarm Rate
-   Detection Lag
-   Residual noise

```{r benchmark-table}
benchmark_all <- results_df |>
  left_join(baseline_summary, by = "sindrome") |>
  group_by(sindrome) |>
  summarise(
    # GAM Metrics
    r2_gam         = 1 - (sum((casi - expected)^2) / sum((casi - mean(casi))^2)),
    rmse_gam       = sqrt(mean((casi - expected)^2)),
    
    # Naive Poisson Metrics (Static Mean)
    r2_poisson     = 1 - (sum((casi - poisson_mean)^2) / sum((casi - mean(casi))^2)),
    rmse_poisson   = sqrt(mean((casi - poisson_mean)^2)),
    
    # Historical Max Metrics (Ceiling Baseline)
    # Note: R2 is often negative for Max lines as they aren't 'means', 
    # but RMSE shows the distance to the 'Safety Ceiling'.
    rmse_hist_max  = sqrt(mean((casi - roll_max_casi)^2)),
    
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), round, 3))

kable(benchmark_all, caption = "System Benchmarking: GAM vs. Static & Historical Baselines") |>
  kable_styling(bootstrap_options = "striped")
```

```{r benchmark-plot}
plot_data_all <- results_df |>
  left_join(baseline_summary, by = "sindrome") |>
  group_by(sindrome) |>
  arrange(Data_inizio_sett) |>
  # Filter for the last 180 days to keep the visual clean
  filter(Data_inizio_sett >= max(Data_inizio_sett) - 180)

ggplot(plot_data_all, aes(x = Data_inizio_sett)) +
  # Actual observed counts (The background signal)
  geom_line(aes(y = casi, color = "Observed"), alpha = 0.4) +
  
  # GAM Threshold (The Seasonal Ceiling)
  geom_line(aes(y = limit_95_A, color = "GAM (Seasonal)"), size = 0.9) +
  
  # Static Poisson Mean (The Simplest Baseline)
  geom_hline(aes(yintercept = poisson_mean, color = "Static Mean"), 
             linetype = "dashed", alpha = 0.7) +
  
  # Historical 2-Year Max (The 'Extreme' Baseline)
  geom_hline(aes(yintercept = roll_max_casi, color = "Hist. 2yr Max"), 
             linetype = "dotted", size = 0.8) +
  
  facet_wrap(~sindrome, scales = "free_y", ncol = 3) +
  
  scale_color_manual(values = c(
    "Observed" = "gray70", 
    "GAM (Seasonal)" = "#E41A1C",   # Red
    "Static Mean" = "#4DAF4A",      # Green
    "Hist. 2yr Max" = "#377EB8"     # Blue
  )) +
  
  theme_minimal(base_size = 11) +
  theme(legend.position = "bottom",
        strip.text = element_text(face = "bold"),
        panel.grid.minor = element_blank()) +
  labs(
    title = "Comparison of Surveillance Thresholds",
    subtitle = "GAM seasonal thresholds vs. static and historical extreme baselines (Last 180 Days)",
    x = "Date",
    y = "Daily Case Volume",
    color = "Threshold Type"
  )
```

## Dashboard and Visualisation

The final stage of the surveillance system translates statistical outputs into an actionable decision support system. This visualization layer allows public health officials to immediately identify which syndromes are deviating from their expected baselines.

### Current Alert Status

The summary table displays the most recent week of data for all nine syndromes. It provides a side-by-side comparison of Approach A (Probability) and Approach B (Magnitude).

-   Exp (Expected): The baseline volume predicted by the GAM, accounting for long-term trends and seasonality.
-   Std_Resid (Sigma Score): The number of standard deviations the current volume sits above the mean.
-   Status A & B: Color-coded indicators that categorize the risk level. A "Double Red" across both columns indicates the highest level of conviction for a potential outbreak.
-   Conf % (Confidence Level): This represents the statistical probability that the observed increase is not due to random chance. A confidence level of 99% indicates that a spike of this magnitude occurs less than 1% of the time under normal conditions, providing a clear mathematical justification for the alert status.

```{r dashboard}
# 1. Prepare the comparison table with the missing conf_level column
comparison_tab <- results_df |>
  group_by(sindrome) |>
  slice_tail(n = 1) |>
  mutate(
    # Calculate Confidence %: how extreme is this observation?
    # (1 - p_val) * 100 gives us the percentile ranking
    `Conf_%` = round((1 - p_val) * 100, 2)
  ) |>
  select(sindrome, 
         Date = Data_inizio_sett, 
         Cases = casi, 
         Exp = expected, 
         Std_Resid = std_residual, 
         `Conf_%`, 
         Status_A = status_A, 
         Status_B = status_B) |>
  mutate(across(where(is.numeric), round, 2))

# 2. Render the Kable table
kable(comparison_tab, caption = "Latest Week Anomaly Detection") |>
  kable_styling(bootstrap_options = c("striped", "hover")) |>
  # Format Confidence column
  column_spec(6, bold = TRUE, 
              color = ifelse(comparison_tab$`Conf_%` >= 99, "red", "black")) |>
  # Approach A Status
  column_spec(7, color = "white", bold = TRUE,
              background = case_when(
                comparison_tab$Status_A == "RED" ~ "#d62728",
                comparison_tab$Status_A == "YELLOW" ~ "#ff7f0e",
                TRUE ~ "#2ca02c"
              )) |>
  # Approach B Status
  column_spec(8, color = "white", bold = TRUE,
              background = case_when(
                comparison_tab$Status_B == "RED" ~ "#d62728",
                comparison_tab$Status_B == "YELLOW" ~ "#ff7f0e",
                TRUE ~ "#2ca02c"
              ))
```

### Time-Series Analysis: The Sigma Chart

While the table provides a snapshot, the time-series chart reveals the duration and evolution of anomalies. By plotting the Sigma Score over time, the system distinguishes between isolated spikes (one-off outliers) and sustained clusters (indicative of a developing epidemic).

-   The Baseline (0.0): Represents a week where observed cases perfectly match the model's prediction.
-   Threshold Lines: The dashed orange (1.96σ) and red (2.57σ) lines act as permanent benchmarks for 95% and 99% confidence, respectively.
-   Stability Check: Periods where the "Green" points hug the zero line indicate times of high model accuracy and low public health activity.

```{r sigma-chart}
ggplot(results_df, aes(x = Data_inizio_sett, y = std_residual)) +
  geom_line(color = "grey80", alpha = 0.5) +
  # Fixed: No mapping of 'color' inside geom_hline to avoid aesthetic errors
  geom_hline(yintercept = 1.96, linetype = "dashed", color = "orange") +
  geom_hline(yintercept = 2.57, linetype = "dashed", color = "red") +
  geom_point(aes(color = status_B), size = 1.5) +
  scale_color_manual(values = c("GREEN" = "seagreen", "YELLOW" = "orange", "RED" = "firebrick")) +
  facet_wrap(~sindrome, ncol = 3, scales = "free_y") +
  theme_minimal() +
  labs(title = "Standardized Residuals (Sigma Score) Over Time",
       subtitle = "Orange line = 95% threshold (1.96σ); Red line = 99% threshold (2.57σ)",
       y = "Sigma Score", x = "Date", color = "Alert Status")
```

## Forecasting

The surveillance system employs a Hybrid Adaptive Forecast to establish thresholds for the upcoming period. This approach is specifically tuned for Burst Detection: it preserves long-term seasonal patterns but prioritizes recent local trends to isolate sudden anomalies from broad epidemic waves.

Seasonal Geometry: The model utilizes General Additive Model (GAM) splines to project the background cyclical patterns (e.g., the historical decline of respiratory cases in late winter).

14-Day Adaptive Calibration: To account for the "current reality" without being overwhelmed by seasonal noise, the model calculates a Bias Factor based on the trailing 14 days of observed data. This shorter window allows the model to "absorb" steady seasonal rises quickly, ensuring that alerts are triggered only by sudden, sharp deviations.

Threshold Projection: This factor "shifts" the seasonal curve vertically. The Green, Yellow, and Red risk zones are thus calibrated to the current volume, acting as a high-pass filter that ignores the "slow" seasonal trend to catch "fast" localized bursts.

Note on Reliability: While the model projects a 4-week horizon, the statistical confidence is highest for the immediate 7-day window. The 14-day shift remains static in the projection; therefore, if a new baseline shift occurs mid-month, the later thresholds will require a manual update or a new data refresh.

```{r forecast-table}
# 1. Create the base forecast grid (Future 30 days)
last_obs_date <- max(df_long$Data_inizio_sett, na.rm = TRUE)
forecast_dates <- seq(last_obs_date + days(1), last_obs_date + days(30), by = "day")

forecast_df <- map_dfr(names(models_list), function(s) {
  new_data <- tibble(
    Data_inizio_sett = forecast_dates,
    sindrome = s,
    week_num = as.integer(as.numeric(Data_inizio_sett - min(df_long$Data_inizio_sett)) / 7),
    week_of_year = week(Data_inizio_sett),
    dow = wday(Data_inizio_sett, label = TRUE)
  )
  new_data$expected <- as.numeric(predict(models_list[[s]], newdata = new_data, type = "response"))
  return(new_data)
})

# 2. Get Thetas and 14-day Bias
thetas_df <- map_df(names(models_list), function(s) {
  tibble(sindrome = s, theta_val = models_list[[s]]$family$getTheta(TRUE))
})

baseline_shift_14 <- results_df |>
  group_by(sindrome) |>
  slice_tail(n = 14) |> 
  summarise(bias_factor = mean(casi) / mean(expected), .groups = "drop")

# 3. Apply Calibration
forecast_df <- forecast_df |>
  left_join(baseline_shift_14, by = "sindrome") |>
  left_join(thetas_df, by = "sindrome") |>
  mutate(
    expected_shifted = expected * bias_factor,
    limit_95 = qnbinom(0.95, mu = expected_shifted, size = theta_val),
    limit_99 = qnbinom(0.99, mu = expected_shifted, size = theta_val)
  )

# 4. Table Output
next_week_forecast <- forecast_df |>
  filter(Data_inizio_sett <= last_obs_date + days(7)) |>
  group_by(sindrome) |>
  summarise(
    `Next Week` = paste(format(min(Data_inizio_sett), "%d %b"), "-", format(max(Data_inizio_sett), "%d %b")),
    `Exp. Cases` = round(mean(expected_shifted), 1),
    `Yellow (95%)` = round(mean(limit_95), 1),
    `Red (99%)` = round(mean(limit_99), 1),
    .groups = "drop"
  )

kable(next_week_forecast, caption = "Adaptive Thresholds: Forecast for the Immediate 7-Day Window") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

```{r forecast-plot}
# 1. To see the black line, we must bind the observed data WITH the forecast thresholds
history_plot <- results_df |>
  filter(Data_inizio_sett > last_obs_date - days(30)) |> # Show last month of history
  select(Data_inizio_sett, sindrome, casi, expected, limit_95 = limit_95_A, limit_99 = limit_99_A) |>
  mutate(type = "Observed")

future_plot <- forecast_df |>
  select(Data_inizio_sett, sindrome, expected = expected_shifted, limit_95, limit_99) |>
  mutate(casi = NA, type = "Forecast")

plot_ready <- bind_rows(history_plot, future_plot)

# 2. Final Plot
ggplot(plot_ready, aes(x = Data_inizio_sett)) +
  geom_ribbon(aes(ymin = 0, ymax = limit_95, fill = "Safe (Green)"), alpha = 0.2) +
  geom_ribbon(aes(ymin = limit_95, ymax = limit_99, fill = "Alert (Yellow)"), alpha = 0.4) +
  geom_ribbon(aes(ymin = limit_99, ymax = limit_99 * 1.5, fill = "Critical (Red)"), alpha = 0.2) +
  geom_line(aes(y = casi, color = "Observed Cases"), linewidth = 0.8, na.rm = TRUE) +
  geom_vline(xintercept = as.numeric(last_obs_date), linetype = "dashed", color = "gray40") +
  annotate("text", x = last_obs_date, y = Inf, label = "Today ", vjust = 2, hjust = 1.1, size = 3.5, fontface = "italic") +
  facet_wrap(~sindrome, scales = "free_y", ncol = 2) +
  scale_fill_manual(values = c("Safe (Green)" = "#2ca02c", "Alert (Yellow)" = "#ff7f0e", "Critical (Red)" = "#d62728")) +
  scale_color_manual(values = c("Observed Cases" = "black")) +
  theme_minimal() +
  labs(title = "Adaptive Surveillance Horizon: Burst Detection Mode",
       subtitle = "Thresholds adjusted via 14-day baseline shift to isolate short-term anomalies.",
       x = "Reporting Period", y = "Daily ED Visits", fill = "Threshold Level") +
  theme(legend.position = "bottom")
```

## Conclusion

This framework establishes a specialized health protection and surveillance system tailored for the complexities of public health data. By utilizing Generalized Additive Models (GAMs), the system successfully captures non-linear trends and seasonal cycles that simpler models often miss. This increased precision directly benefits users by providing more accurate thresholds, significantly reducing the gap between statistical alerts and true epidemiological threats.

### Future developments

The transition from a reactive multi-year baseline to a dynamic GAM framework has already provided a seasonally-aware, statistically grounded "ceiling" for emergency department activity. To evolve this into a truly predictive health protection tool, the focus should shift toward integrating spatial clustering to verify local signals and conducting simulation-based stress tests to quantify exactly how small of an outbreak the system can catch.
